# TrainingArguments:
#     output_dir: "outputs"
#     num_train_epochs: 2                     # Slightly longer training
#     warmup_steps: 100                        # Reduced warmup for faster start
#     per_device_train_batch_size: 1           # Use 4 if GPU/CPU allows, for faster training
#     per_device_eval_batch_size: 4
#     weight_decay: 0.01
#     logging_steps: 20                        # Log less frequently
#     evaluation_strategy: "steps"             # Enables eval_steps to work
#     eval_steps: 100                          # More frequent evaluation to monitor
#     save_steps: 200                          # Save checkpoints regularly
#     gradient_accumulation_steps: 8           # More frequent updates
#     save_total_limit: 2                      # Keep only 2 checkpoints
#     fp16: False                               # Use if you have GPU with mixed precision
#     load_best_model_at_end: True


# TrainingArguments:
#   num_train_epochs: 1
#   warmup_steps: 500
#   per_device_train_batch_size: 1
#   weight_decay: 0.01
#   logging_steps: 10
#   eval_steps: 500
#   save_steps: 1e6
#   gradient_accumulation_steps: 16

TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 500
  per_device_train_batch_size: 3
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: "steps"
  eval_steps: 300
  save_steps: 1000
  gradient_accumulation_steps: 8